# ğŸ“˜ Les Pipelines et la MÃ©moire dans LangChain

## ğŸ§ Qu'est-ce qu'un Pipeline en IA ?
Un **pipeline IA** est une chaÃ®ne de traitements permettant d'organiser et d'automatiser le passage de donnÃ©es d'une Ã©tape Ã  une autre. Dans **LangChain**, un pipeline structure le flux dâ€™informations entre diffÃ©rents composants, comme :
- **Les modÃ¨les de langage (LLM)** ğŸ§ 
- **Les outils (moteurs de recherche, bases de donnÃ©es, API, etc.)** ğŸ› 
- **Les mÃ©moires contextuelles** ğŸ’¾

GrÃ¢ce aux pipelines, on peut **construire des agents IA plus intelligents et plus efficaces**.

---

## âš™ï¸ Comment fonctionne un pipeline LangChain ?
Un pipeline LangChain se compose de plusieurs Ã©lÃ©ments :
1. **Un modÃ¨le de langage** (ex: Mistral, Llama, Falcon)
2. **Un ou plusieurs outils** pour rÃ©cupÃ©rer des donnÃ©es externes (API open-source, vector stores, etc.)
3. **Un mÃ©canisme de mÃ©moire** pour stocker lâ€™historique des interactions

### ğŸ”¹ Exemple d'un pipeline basique
```
Utilisateur â†’ [MÃ©moire] â†’ [LLM] â†’ [Outil de recherche] â†’ [RÃ©ponse gÃ©nÃ©rÃ©e]
```
Chaque Ã©tape reÃ§oit des donnÃ©es, les transforme et les transmet Ã  lâ€™Ã©tape suivante.

---

## ğŸ”— IntÃ©gration de la MÃ©moire dans un Agent IA
Par dÃ©faut, un LLM ne retient pas le contexte entre deux interactions. **LangChain permet dâ€™ajouter une mÃ©moire** pour :
- **Stocker les conversations passÃ©es**
- **Rappeler des informations pertinentes**
- **Rendre les interactions plus naturelles**

### ğŸ“Œ Types de MÃ©moires dans LangChain
1. **MÃ©moire courte** (conversationnelle) : Rappelle les derniers Ã©changes.
2. **MÃ©moire longue** (vectorielle) : Stocke et retrouve des informations Ã  long terme.
3. **MÃ©moire basÃ©e sur des fichiers** : Sauvegarde les interactions dans une base locale.

---

## ğŸ“ ImplÃ©mentation d'un Pipeline de Base
Nous allons maintenant crÃ©er un **pipeline minimal** en LangChain avec un LLM open-source (**Mistral**) et une mÃ©moire conversationnelle.

### ğŸ— Version 1 : Utilisation de lâ€™API de Mistral
```python
from langchain.llms import MistralAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
import os

# DÃ©finir votre clÃ© API de Mistral (Ã  ajouter dans un fichier .env)
api_key = os.getenv("MISTRAL_API_KEY")
llm = MistralAI(api_key=api_key)

# CrÃ©ation de la mÃ©moire
memory = ConversationBufferMemory()

# Construction du pipeline
conversation = ConversationChain(
    llm=llm,
    memory=memory
)

# Test du pipeline
print(conversation.predict(input="Bonjour, qui es-tu ?"))
print(conversation.predict(input="Peux-tu me rappeler ce que je viens de dire ?"))
```

### ğŸ— Version 2 : Utilisation du modÃ¨le en local
#### ğŸ“¥ TÃ©lÃ©charger Mistral 7B GGUF
Avant de pouvoir l'utiliser localement, il faut tÃ©lÃ©charger le modÃ¨le via Hugging Face. Pour simplifier cela, nous avons ajoutÃ© un fichier `download_models.py`, qui permet de tÃ©lÃ©charger automatiquement tous les modÃ¨les nÃ©cessaires.

ExÃ©cutez la commande suivante pour tÃ©lÃ©charger le modÃ¨le localement :
```bash
python -m src.utils.download_models
```

#### ğŸ”¹ Code pour exÃ©cuter le modÃ¨le en local
```python
from langchain.llms import CTransformers
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# Initialisation du modÃ¨le local Mistral 7B GGUF
llm = CTransformers(model="models/mistral/mistral-7b-instruct-v0.1.Q4_K_M.gguf")

# CrÃ©ation de la mÃ©moire
memory = ConversationBufferMemory()

# Construction du pipeline
conversation = ConversationChain(
    llm=llm,
    memory=memory
)

# Test du pipeline
print(conversation.predict(input="Bonjour, qui es-tu ?"))
print(conversation.predict(input="Peux-tu me rappeler ce que je viens de dire ?"))
```

---

## âœ… Explication du Code
- **Version API** : Utilise lâ€™API Mistral avec une clÃ© API.
- **Version Locale** : TÃ©lÃ©charge un modÃ¨le **open-source** et lâ€™exÃ©cute localement via `CTransformers`.
- **Utilisation de `download_models.py`** : Ce script permet de rÃ©cupÃ©rer les modÃ¨les automatiquement, Ã©vitant les tÃ©lÃ©chargements manuels.
- On ajoute une **mÃ©moire conversationnelle** pour conserver le contexte.
- On crÃ©e une **chaÃ®ne de conversation** (`ConversationChain`) pour gÃ©rer lâ€™interaction.
- Lorsquâ€™on pose une question, la mÃ©moire permet au LLM de **se souvenir du contexte**.

---

## ğŸ”¥ Mise Ã  Jour de `main.py`
Pour intÃ©grer et tester le pipeline au dÃ©marrage du projet, nous avons mis Ã  jour `main.py` afin qu'il exÃ©cute un test du modÃ¨le lors de l'initialisation.

ExÃ©cutez :
```bash
python -m src.main
```
Cela affichera un message indiquant que le pipeline fonctionne correctement et montrera la rÃ©ponse du modÃ¨le.

---

## ğŸš€ Prochaine Ã‰tape
Dans le prochain cours, nous verrons **comment crÃ©er et gÃ©rer des agents IA avancÃ©s** avec LangChain et CrewAI.

ğŸ’¡ **PrÃªt Ã  concevoir des agents intelligents ?** ğŸ‘‰ Direction `03_agents.md` !